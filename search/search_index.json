{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Eve-evalkit","text":"<p>Welcome to the Wve-evalkit documentation. This framework provides comprehensive tools for evaluating language models on Earth Observation (EO) specific tasks and benchmarks.</p>"},{"location":"#what-is-eve-evalkit","title":"What is Eve-evalkit?","text":"<p>Eve-evalkit is built on top of the EleutherAI Language Model Evaluation Harness, providing:</p> <ul> <li>Custom EO Tasks: Specialized evaluation tasks for Earth Observation domain, including summarization, MCQA, hallucination detection, and more</li> <li>Full LM-Eval-Harness Support: Access to all standard benchmarks (MMLU-Pro, GSM8K, HellaSwag, etc.)</li> <li>WandB Integration: Automatic experiment tracking and metric logging</li> <li>Flexible Configuration: YAML-based configuration for easy experiment management</li> <li>Production Ready: Built for evaluating models via API endpoints with concurrent requests</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started: Installation, configuration, and running your first evaluation</li> <li>EO Tasks: Detailed information about Earth Observation evaluation tasks</li> <li>Code Reference: API documentation and code examples</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#earth-observation-tasks","title":"Earth Observation Tasks","text":"<p>Evaluate models on specialized EO capabilities:</p> <ul> <li>Summarization: Generate concise summaries of scientific EO documents</li> <li>Multiple-Choice QA: Single and multiple-answer questions from EO curricula</li> <li>Open-Ended QA: Free-form question answering with and without context</li> <li>Hallucination Detection: Identify fabricated or unsupported information</li> <li>Refusal Testing: Assess appropriate refusal behavior when context is insufficient</li> </ul>"},{"location":"#comprehensive-metrics","title":"Comprehensive Metrics","text":"<ul> <li>LLM-as-Judge: Sophisticated evaluation using judge models</li> <li>Traditional Metrics: Accuracy, F1, Precision, Recall, IoU</li> <li>Semantic Metrics: BERTScore, Cosine Similarity</li> <li>Generation Metrics: BLEU, ROUGE for summarization tasks</li> </ul>"},{"location":"#production-features","title":"Production Features","text":"<ul> <li>API Model Support: Evaluate models via OpenAI-compatible endpoints</li> <li>Concurrent Requests: Speed up evaluations with parallel API calls</li> <li>Timeout Handling: Graceful handling of slow or failed requests</li> <li>Result Logging: Comprehensive JSON and JSONL outputs</li> <li>WandB Integration: Track experiments and visualize metrics</li> </ul>"},{"location":"#example-use-cases","title":"Example Use Cases","text":""},{"location":"#research-development","title":"Research &amp; Development","text":"<ul> <li>Benchmark Earth Observation models against established tasks</li> <li>Compare model performance across different architectures</li> <li>Identify strengths and weaknesses in EO domain understanding</li> </ul>"},{"location":"#model-selection","title":"Model Selection","text":"<ul> <li>Evaluate multiple models on EO-specific capabilities</li> <li>Compare general-purpose models vs. domain-specific models</li> <li>Assess trade-offs between performance and cost</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Getting Started: Installation, configuration, and basic usage</li> <li>EO Tasks: Detailed task descriptions, metrics, and examples</li> <li>Code Reference: API documentation and programmatic usage</li> </ul>"},{"location":"#support-contributing","title":"Support &amp; Contributing","text":"<ul> <li>GitHub: eve-esa/eve-evaluation</li> <li>Issues: Report bugs or request features on GitHub</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use this evaluation framework in your research, please cite:</p> <pre><code>@misc{eve2025,\n  title={EVE: Earth Virtual Expert},\n  author={ESA},\n  year={2025},\n  publisher={HuggingFace},\n  url={https://huggingface.co/eve-esa/eve_v0.1}\n}\n</code></pre> <p>For the underlying evaluation harness:</p> <pre><code>@software{eval-harness,\n  author       = {Gao, Leo and others},\n  title        = {A framework for few-shot language model evaluation},\n  year         = 2021,\n  publisher    = {Zenodo},\n  doi          = {10.5281/zenodo.5371628},\n  url          = {https://doi.org/10.5281/zenodo.5371628}\n}\n</code></pre>"},{"location":"eo_tasks/","title":"Earth Observation Evaluation Tasks","text":"<p>This page provides a comprehensive overview of all available Earth Observation (EO) evaluation tasks in eve-evalkit. Each task is designed to assess different capabilities of language models in the Earth Observation domain.</p>"},{"location":"eo_tasks/#quick-reference","title":"Quick Reference","text":"Task Name Type Dataset Size Primary Metrics MCQA Multiple Answer Multiple Choice eve-esa/eve-is-mcqa 432 IoU, Accuracy MCQA Single Answer Multiple Choice eve-esa/mcqa-single-answer 1308 Accuracy Open Ended Generation eve-esa/open-ended 1304 LLM as Judge Open Ended with Context Generation eve-esa/open-ended-w-context 606 LLM Judge Refusal Generation eve-esa/refusal 946 LLM Judge Hallucination Detection Classification eve-esa/hallucination-detection 2996 Accuracy, Precision, Recall, F1"},{"location":"eo_tasks/#detailed-task-descriptions","title":"Detailed Task Descriptions","text":""},{"location":"eo_tasks/#mcqa-multiple-answer","title":"MCQA Multiple Answer","text":"<p>Task Name: <code>is_mcqa</code> or <code>mcqa_multiple_answer</code></p> <p>Description:</p> <p>EVE-mcqa-multiple-answers consists of multiple-choice questions from Imperative Space MOOC exams where questions may have one or more correct answers. Models must identify all correct options from an arbitrary number of choices, making this a challenging task that requires comprehensive understanding rather than simple fact recall.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: is_mcqa\n    num_fewshot: 2\n    max_tokens: 10000\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/eve-is-mcqa</li> <li>Split: train</li> <li>Size: 432 samples</li> <li>Structure: Each example contains a <code>Question</code>, <code>Answers</code> (list of correct labels), and <code>Choices</code> (list with labels and text)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>IoU (Intersection over Union): Measures partial correctness by calculating the overlap between predicted and correct answer sets. IoU = |Predicted \u2229 Correct| / |Predicted \u222a Correct| (higher is better)</li> <li>Accuracy (Exact Match): Binary score where 1.0 means the predicted answer set exactly matches the correct answer set (higher is better)</li> </ul> <p>Why It's Useful:</p> <p>This task tests a model's comprehensive understanding of EO concepts where multiple aspects or factors may be simultaneously correct. The IoU metric is particularly valuable as it rewards partially correct answers, providing a more nuanced evaluation than simple exact matching. This reflects real-world scenarios where partial knowledge is still valuable.</p> <p>Example:</p> <pre><code>{\n  \"Question\": \"Which bands of Sentinel-2 have 10m resolution?\",\n  \"Answers\": [\"A\", \"B\", \"C\"],\n  \"Choices\": {\n    \"label\": [\"A\", \"B\", \"C\", \"D\"],\n    \"text\": [\"B2 (Blue)\", \"B3 (Green)\", \"B4 (Red)\", \"B8 (NIR)\"]\n  }\n}\n</code></pre>"},{"location":"eo_tasks/#mcqa-single-answer","title":"MCQA Single Answer","text":"<p>Task Name: <code>mcqa_single_answer</code></p> <p>Description:</p> <p>EVE-mcqa-single-answer is a traditional multiple-choice dataset with exactly one correct answer per question. Models must identify the single best option from the provided choices, testing factual knowledge and reasoning abilities in the Earth Observation domain.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: mcqa_single_answer\n    num_fewshot: 2\n    max_tokens: 10000\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/mcqa-single-answer</li> <li>Split: train</li> <li>Size: ~1000 samples</li> <li>Structure: Each example contains a <code>question</code>, <code>choices</code> (list of answer texts), and <code>answer</code> (single letter indicating correct choice)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>Accuracy: Percentage of questions answered correctly (higher is better)</li> </ul> <p>Why It's Useful:</p> <p>This task evaluates factual knowledge and reasoning abilities in scenarios where there is a single definitively correct answer. It's particularly useful for assessing fundamental EO concepts, terminology, and principles. The single-answer format reduces ambiguity and provides clear, interpretable results.</p> <p>Example:</p> <pre><code>{\n  \"question\": \"What is the spatial resolution of Sentinel-2's visible bands?\",\n  \"choices\": [\"5 meters\", \"10 meters\", \"20 meters\", \"60 meters\"],\n  \"answer\": \"B\"  # \"10 meters\"\n}\n</code></pre>"},{"location":"eo_tasks/#open-ended","title":"Open Ended","text":"<p>Task Name: <code>open_ended</code></p> <p>Description:</p> <p>EVE-open-ended is a collection of ~969 open-ended question-answer pairs focused on Earth Observation. The dataset covers a wide range of EO topics including satellite imagery analysis, remote sensing techniques, environmental monitoring, and LiDAR. Models must generate free-form responses demonstrating deep understanding without the constraints of multiple-choice formats.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: open_ended\n    num_fewshot: 5\n    max_tokens: 40000\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/open-ended</li> <li>Split: train</li> <li>Size: ~969 samples</li> <li>Structure: Each example contains a <code>Question</code> and <code>Answer</code> (reference)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>LLM as Judge: A judge model evaluates the quality, accuracy, and completeness of generated answers using strict fact-checking rules (0 = FAIL, 1 = PASS)</li> <li>Alternative metrics: BLEU, ROUGE, Cosine Similarity, BERTScore</li> </ul> <p>LLM Judge Evaluation Rules:</p> <ol> <li>Contradiction Check: Fails if the answer contains ANY fact contradicting the reference</li> <li>Relevance Check: Fails if the answer omits ESSENTIAL technical facts from the reference</li> <li>Additive Information: Additional correct information is acceptable if it doesn't contradict</li> <li>Focus on Substance: Ignores style, length, and tolerates minor phrasing differences</li> </ol> <p>Why It's Useful:</p> <p>This task assesses a model's ability to explain concepts, provide detailed answers, and demonstrate deep understanding. It's essential for evaluating models intended for educational or explanatory applications in EO, where nuanced explanations and technical accuracy are paramount.</p>"},{"location":"eo_tasks/#open-ended-with-context","title":"Open Ended with Context","text":"<p>Task Name: <code>open_ended_w_context</code></p> <p>Description:</p> <p>EVE-open-ended-w-context provides open-ended questions that must be answered using 1-3 accompanying context documents. This tests the model's ability to extract and synthesize information from reference materials, making it ideal for evaluating Retrieval-Augmented Generation (RAG) systems. Not all samples contain all three documents, requiring models to handle variable numbers of context documents gracefully.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: open_ended_w_context\n    num_fewshot: 5\n    max_tokens: 40000\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/open-ended-w-context</li> <li>Split: train</li> <li>Structure: Each example contains a <code>Question</code>, <code>Answer</code>, and up to three context documents (<code>Doc 1</code>, <code>Doc 2</code>, <code>Doc 3</code>)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>LLM Judge: Evaluates whether answers are grounded in the provided context and correctly answer the question (higher is better)</li> <li>Uses the same strict fact-checking evaluation rules as open-ended tasks</li> </ul> <p>Why It's Useful:</p> <p>This task evaluates retrieval-augmented generation (RAG) capabilities, testing whether models can accurately extract information from provided documents rather than relying solely on parametric knowledge. This is crucial for applications where answers must be grounded in specific documentation or data sources. It also tests the model's ability to distinguish between context-provided information and pre-trained knowledge.</p> <p>Example:</p> <pre><code>{\n  \"Question\": \"What is the spatial resolution of Sentinel-2's visible bands?\",\n  \"Answer\": \"Sentinel-2's visible bands have a spatial resolution of 10 meters.\",\n  \"Doc 1\": \"The Sentinel-2 mission comprises a constellation...\",\n  \"Doc 2\": \"Sentinel-2 carries the Multi-Spectral Instrument (MSI)...\",\n  \"Doc 3\": \"\"  # May be empty\n}\n</code></pre>"},{"location":"eo_tasks/#refusal","title":"Refusal","text":"<p>Task Name: <code>refusal</code></p> <p>Description:</p> <p>EVE-Refusal tests whether language models can appropriately refuse to answer questions when the provided context does not contain sufficient information. The dataset presents questions alongside context documents that intentionally lack the necessary information to answer. A well-calibrated model should recognize this limitation and refuse to answer, rather than generating plausible but incorrect information.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: refusal\n    num_fewshot: 5\n    max_tokens: 40000\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/refusal</li> <li>Split: train</li> <li>Structure: Each example contains a <code>question</code> and <code>context</code> (insufficient for answering)</li> <li>Expected Answer: \"I'm sorry, but the provided context does not contain enough information to answer that question.\"</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>LLM Judge: Evaluates whether the model appropriately refuses to answer or acknowledges insufficient information (higher is better)</li> </ul> <p>Expected Behavior:</p> <ul> <li>Recognize when provided context lacks sufficient information</li> <li>Explicitly refuse to answer or state information is not available</li> <li>Avoid generating plausible-sounding but fabricated information</li> <li>Maintain accuracy and honesty over completeness</li> </ul> <p>Why It's Useful:</p> <p>This task tests a critical safety and reliability feature: the ability to recognize limitations and avoid generating potentially incorrect information when context is insufficient. This prevents hallucinations and ensures trustworthy behavior in production systems. It's particularly important for RAG systems and applications where factual accuracy is paramount.</p>"},{"location":"eo_tasks/#hallucination-detection","title":"Hallucination Detection","text":"<p>Task Name: <code>hallucination_detection</code></p> <p>Description:</p> <p>EVE-Hallucination is a specialized dataset for evaluating language models' tendency to hallucinate in the Earth Observation domain. Unlike typical QA datasets, this contains deliberately hallucinated answers with detailed annotations marking which portions of text are hallucinated. The task is to identify whether a given answer contains hallucinated (false or unsupported) information.</p> <p>How to Call:</p> <pre><code>tasks:\n  - name: hallucination_detection\n    num_fewshot: 0\n    max_tokens: 100\n    judge_api_key: !ref judge_api_key\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>Dataset:</p> <ul> <li>Source: eve-esa/hallucination-detection</li> <li>Split: train</li> <li>Structure: Each example contains <code>Question</code>, <code>Answer</code> (with hallucinations), <code>Soft labels</code> (probabilistic spans), and <code>Hard labels</code> (definite spans)</li> </ul> <p>Evaluation Metrics:</p> <ul> <li>Accuracy: Overall correctness of hallucination detection (higher is better)</li> <li>Precision: Ratio of correctly identified hallucinations to all predicted hallucinations (higher is better)</li> <li>Recall: Ratio of correctly identified hallucinations to all actual hallucinations (higher is better)</li> <li>F1 Score: Harmonic mean of precision and recall (higher is better)</li> </ul> <p>Task Levels:</p> <ol> <li>Binary Detection: Determine if answer contains any hallucinated information (yes/no)</li> <li>Hard Span Detection: Identify exact character spans that are hallucinated</li> <li>Soft Span Detection: Identify spans with confidence scores</li> </ol> <p>Why It's Useful:</p> <p>This task evaluates a model's ability to self-assess and identify unreliable or fabricated information in EO contexts. Models with strong hallucination detection capabilities are more trustworthy and can potentially be used to validate outputs from other systems. This is crucial for safety-critical applications like climate monitoring, disaster response, and environmental analysis.</p> <p>Example:</p> <pre><code>{\n  \"Question\": \"What is the spatial resolution of Sentinel-2's visible bands?\",\n  \"Answer\": \"Sentinel-2's visible bands have a spatial resolution of 5 meters, making it the highest resolution freely available satellite.\",\n  \"Hard labels\": [[52, 60], [73, 127]]  # Character spans that are hallucinated\n}\n</code></pre>"},{"location":"eo_tasks/#running-tasks","title":"Running Tasks","text":""},{"location":"eo_tasks/#using-configuration-file","title":"Using Configuration File","text":"<p>Add tasks to your <code>evals.yaml</code>:</p> <pre><code>constants:\n  judge_api_key: your-judge-api-key\n  judge_base_url: https://openrouter.ai/api/v1\n  judge_name: mistralai/mistral-large-2411\n  tasks:\n    - name: eo_summarization\n      num_fewshot: 0\n      max_tokens: 20000\n      judge_api_key: !ref judge_api_key\n      judge_base_url: !ref judge_base_url\n      judge_name: !ref judge_name\n    - name: is_mcqa\n      num_fewshot: 2\n      max_tokens: 10000\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n\nmodels:\n  - name: your-model-name\n    base_url: https://api.provider.com/v1/chat/completions\n    api_key: your-api-key\n    temperature: 0.1\n    num_concurrent: 5\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre> <p>Then run:</p> <pre><code>python scripts/evaluate.py evals.yaml\n</code></pre>"},{"location":"eo_tasks/#direct-command-line","title":"Direct Command Line","text":"<pre><code>lm_eval --model openai-chat-completions \\\n        --model_args base_url=https://api.provider.com,model=model-name,num_concurrent=5 \\\n        --tasks {task_name} \\\n        --include tasks \\\n        --num_fewshot 0 \\\n        --output_path ./outputs \\\n        --log_samples \\\n        --apply_chat_template\n</code></pre> <p>For tasks using LLM-as-judge metrics, set environment variables:</p> <pre><code>export JUDGE_API_KEY=your-judge-api-key\nexport JUDGE_BASE_URL=https://api.provider.com/v1\nexport JUDGE_NAME=judge-model-name\n</code></pre>"},{"location":"eo_tasks/#task-selection-guide","title":"Task Selection Guide","text":"<p>Choose tasks based on your evaluation goals:</p> <p>Factual Knowledge: - <code>mcqa_single_answer</code> - Single correct answer questions - <code>is_mcqa</code> - Multiple correct answers with partial credit - <code>wiley_mcqa</code> - Standardized textbook questions</p> <p>Generation Quality: - <code>eo_summarization</code> - Abstractive summarization of technical content - <code>open_ended</code> - Free-form explanatory answers</p> <p>Grounded Generation (RAG): - <code>open_ended_w_context</code> - Answer questions using provided documents - <code>refusal</code> - Recognize when context is insufficient</p> <p>Reliability &amp; Safety: - <code>hallucination_detection</code> - Identify fabricated information - <code>refusal</code> - Avoid answering without sufficient information</p> <p>Comprehensive Evaluation: - Run all tasks for a complete assessment across different capabilities</p>"},{"location":"eo_tasks/#evaluation-best-practices","title":"Evaluation Best Practices","text":"<ol> <li>Use Few-Shot Examples: Most tasks benefit from few-shot examples (typically 2-5) to demonstrate the expected format</li> <li>Set Appropriate Timeouts: Some tasks require longer generation (e.g., summarization), so adjust timeouts accordingly</li> <li>Configure Judge Model: For LLM-as-judge tasks, choose a capable judge model (e.g., GPT-4, Claude 3.5 Sonnet, Mistral Large)</li> <li>Log Samples: Always use <code>--log_samples</code> to inspect individual predictions and understand model behavior</li> <li>Monitor Costs: LLM-as-judge evaluation can be expensive; consider using smaller subsets for initial testing</li> </ol>"},{"location":"eo_tasks/#additional-resources","title":"Additional Resources","text":"<ul> <li>Dataset Repository: https://huggingface.co/eve-esa</li> <li>GitHub Repository: https://github.com/eve-esa/eve-evaluation</li> <li>LM Evaluation Harness: https://github.com/EleutherAI/lm-evaluation-harness</li> </ul>"},{"location":"eo_tasks/#citation","title":"Citation","text":"<p>If you use these tasks or datasets in your research, please cite:</p> <pre><code>@misc{eve2025,\n  title={EVE: Earth Virtual Expert},\n  author={ESA},\n  year={2025},\n  publisher={HuggingFace},\n  url={https://huggingface.co/eve-esa/eve_v0.1}\n}\n</code></pre> <p>For the underlying evaluation framework:</p> <pre><code>@software{eval-harness,\n  author       = {Gao, Leo and others},\n  title        = {A framework for few-shot language model evaluation},\n  month        = sep,\n  year         = 2021,\n  publisher    = {Zenodo},\n  version      = {v0.0.1},\n  doi          = {10.5281/zenodo.5371628},\n  url          = {https://doi.org/10.5281/zenodo.5371628}\n}\n</code></pre>"},{"location":"evalkit/","title":"Evalkit","text":"<p>Metrics module for EVE evaluation.</p>"},{"location":"evalkit/#metrics.LoggableFuture","title":"<code>LoggableFuture</code>","text":"<p>Wrapper around Future that provides a nicer string representation for logging.</p> Source code in <code>metrics/judge_utils.py</code> <pre><code>class LoggableFuture:\n    \"\"\"Wrapper around Future that provides a nicer string representation for logging.\"\"\"\n\n    def __init__(self, future: Future):\n        self._future = future\n\n    def result(self, timeout=None):\n        \"\"\"Get the result from the underlying future.\"\"\"\n        return self._future.result(timeout=timeout)\n\n    def __repr__(self):\n        \"\"\"Return a nicer representation for logging.\"\"\"\n        if self._future.done():\n            try:\n                return str(self._future.result())\n            except Exception as e:\n                return f\"&lt;Failed: {type(e).__name__}&gt;\"\n        else:\n            return \"&lt;Pending LLM judge evaluation&gt;\"\n\n    def __str__(self):\n        return self.__repr__()\n</code></pre>"},{"location":"evalkit/#metrics.LoggableFuture.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a nicer representation for logging.</p> Source code in <code>metrics/judge_utils.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a nicer representation for logging.\"\"\"\n    if self._future.done():\n        try:\n            return str(self._future.result())\n        except Exception as e:\n            return f\"&lt;Failed: {type(e).__name__}&gt;\"\n    else:\n        return \"&lt;Pending LLM judge evaluation&gt;\"\n</code></pre>"},{"location":"evalkit/#metrics.LoggableFuture.result","title":"<code>result(timeout=None)</code>","text":"<p>Get the result from the underlying future.</p> Source code in <code>metrics/judge_utils.py</code> <pre><code>def result(self, timeout=None):\n    \"\"\"Get the result from the underlying future.\"\"\"\n    return self._future.result(timeout=timeout)\n</code></pre>"},{"location":"evalkit/#metrics.aggregate_llm_judge","title":"<code>aggregate_llm_judge(items)</code>","text":"<p>Aggregate LLM judge results by waiting for futures and calculating mean.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Union[List[LoggableFuture], List[Future]]</code> <p>List of LoggableFuture or Future objects containing scores.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mean score across all items, or 0.0 if items is empty.</p> Source code in <code>metrics/judge_utils.py</code> <pre><code>def aggregate_llm_judge(items: Union[List[LoggableFuture], List[Future]]) -&gt; float:\n    \"\"\"\n    Aggregate LLM judge results by waiting for futures and calculating mean.\n\n    Args:\n        items: List of LoggableFuture or Future objects containing scores.\n\n    Returns:\n        Mean score across all items, or 0.0 if items is empty.\n    \"\"\"\n    if not items:\n        return 0.0\n\n    # Handle both LoggableFuture and regular Future objects\n    scores = []\n    for item in items:\n        if isinstance(item, LoggableFuture):\n            scores.append(item.result())\n        else:\n            scores.append(item.result())\n\n    return mean(scores) if scores else 0.0\n</code></pre>"},{"location":"evalkit/#metrics.judge_qa_with_llm","title":"<code>judge_qa_with_llm(sample, model_name=None, api_key=None, base_url=None)</code>","text":"<p>Calls the LLM judge for QA evaluation with binary (0/1) scoring.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict</code> <p>Dictionary with \"question\", \"output\", and \"reference\" keys.</p> required <code>model_name</code> <code>Optional[str]</code> <p>Name of the judge model. If None, reads from JUDGE_NAME env var.</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for the judge. If None, uses environment variables.</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Base URL for the API. If None, uses environment variables.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Integer score: 1 for correct, 0 for incorrect.</p> Source code in <code>metrics/judge_utils.py</code> <pre><code>def judge_qa_with_llm(\n    sample: dict,\n    model_name: Optional[str] = None,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n) -&gt; int:\n    \"\"\"\n    Calls the LLM judge for QA evaluation with binary (0/1) scoring.\n\n    Args:\n        sample: Dictionary with \"question\", \"output\", and \"reference\" keys.\n        model_name: Name of the judge model. If None, reads from JUDGE_NAME env var.\n        api_key: API key for the judge. If None, uses environment variables.\n        base_url: Base URL for the API. If None, uses environment variables.\n\n    Returns:\n        Integer score: 1 for correct, 0 for incorrect.\n    \"\"\"\n    if model_name is None:\n        model_name = os.getenv(\"JUDGE_NAME\") or os.getenv(\"JUDGE_MODEL\") or \"mistral-large-latest\"\n\n    client = get_judge_client(api_key=api_key, base_url=base_url)\n    prompt_template = get_qa_prompt_template()\n\n    # JSON schema for structured output (OpenAI-style)\n    json_schema = {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"judge_score\",\n            \"strict\": True,\n            \"schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"score\": {\n                        \"type\": \"integer\",\n                        \"enum\": [0, 1],\n                        \"description\": \"1 if correct, 0 if incorrect\",\n                    }\n                },\n                \"required\": [\"score\"],\n                \"additionalProperties\": False,\n            },\n        },\n    }\n\n    # Simple schema for format instructions\n    simple_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"score\": {\n                \"type\": \"integer\",\n                \"enum\": [0, 1],\n                \"description\": \"1 if correct, 0 if incorrect.\",\n            }\n        },\n        \"required\": [\"score\"],\n    }\n\n    format_instructions = (\n        \"You must respond with a JSON object that strictly follows this schema:\\n\"\n        f\"{json.dumps(simple_schema, indent=2)}\"\n    )\n\n    final_prompt = prompt_template.format(\n        question=sample[\"question\"],\n        output=sample[\"output\"],\n        reference=sample[\"reference\"],\n        format_instructions=format_instructions,\n    )\n\n    try:\n        # Try structured outputs first (OpenAI-compatible APIs)\n        try:\n            response = client.chat.completions.create(\n                model=model_name,\n                messages=[{\"role\": \"user\", \"content\": final_prompt}],\n                response_format=json_schema,\n                temperature=0.0,\n                max_tokens=100,\n            )\n        except Exception:\n            # Fall back to basic JSON mode if structured outputs not supported\n            response = client.chat.completions.create(\n                model=model_name,\n                messages=[{\"role\": \"user\", \"content\": final_prompt}],\n                response_format={\"type\": \"json_object\"},\n                temperature=0.0,\n                max_tokens=100,\n            )\n\n        response_content = response.choices[0].message.content\n        if response_content is None:\n            raise ValueError(\"LLM response content is None\")\n\n        data = json.loads(response_content)\n        score = data.get(\"score\")\n\n        if score in [0, 1]:\n            return int(score)\n        else:\n            print(f\"Warning: Judge returned invalid score: {score}. Defaulting to 0.\")\n            print(f\"Full response: {response_content}\")\n            print(f\"Question: {sample['question'][:100]}...\")\n            print(f\"Output: {sample['output'][:100]}...\")\n            return 0\n\n    except Exception as e:\n        print(f\"Error during LLM judge call: {e}. Defaulting to score 0.\")\n        print(f\"Question: {sample['question'][:100]}...\")\n        return 0\n</code></pre>"},{"location":"evalkit/#metrics.process_qa_results","title":"<code>process_qa_results(doc, results, question_key='question', answer_key='answer', sleep_time=0.0)</code>","text":"<p>Process QA results with LLM judge in background thread.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>dict</code> <p>Document dictionary containing the question and reference answer.</p> required <code>results</code> <code>list[str]</code> <p>List of model outputs (first element is used).</p> required <code>question_key</code> <code>str</code> <p>Key in doc for the question text.</p> <code>'question'</code> <code>answer_key</code> <code>str</code> <p>Key in doc for the reference answer.</p> <code>'answer'</code> <code>sleep_time</code> <code>float</code> <p>Optional delay before submitting (for rate limiting).</p> <code>0.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with \"llm_as_judge\" key containing LoggableFuture.</p> Source code in <code>metrics/judge_utils.py</code> <pre><code>def process_qa_results(\n    doc: dict,\n    results: list[str],\n    question_key: str = \"question\",\n    answer_key: str = \"answer\",\n    sleep_time: float = 0.0,\n) -&gt; dict:\n    \"\"\"\n    Process QA results with LLM judge in background thread.\n\n    Args:\n        doc: Document dictionary containing the question and reference answer.\n        results: List of model outputs (first element is used).\n        question_key: Key in doc for the question text.\n        answer_key: Key in doc for the reference answer.\n        sleep_time: Optional delay before submitting (for rate limiting).\n\n    Returns:\n        Dictionary with \"llm_as_judge\" key containing LoggableFuture.\n    \"\"\"\n    sample = {\n        \"question\": doc[question_key],\n        \"output\": results[0],\n        \"reference\": doc[answer_key],\n    }\n\n    future = _EXECUTOR.submit(judge_qa_with_llm, sample)\n\n    if sleep_time &gt; 0:\n        time.sleep(sleep_time)\n\n    return {\"llm_as_judge\": LoggableFuture(future)}\n</code></pre>"},{"location":"getting_started/","title":"Getting Started with eve-evalkit","text":"<p>eve-evalkit is built on top of the EleutherAI Language Model Evaluation Harness, which means it supports all tasks available in the lm-evaluation-harness in addition to the custom Earth Observation tasks.</p>"},{"location":"getting_started/#quick-start","title":"Quick Start","text":""},{"location":"getting_started/#1-installation","title":"1. Installation","text":"<p>Follow the installation instructions in the README:</p> <pre><code># Clone the repository\ngit clone https://github.com/eve-esa/eve-evaluation.git\ncd eve-evaluation\n\n# Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment and install dependencies\nuv venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\nuv sync\n</code></pre>"},{"location":"getting_started/#2-running-evaluations","title":"2. Running Evaluations","text":"<p>The recommended way to run evaluations is using the YAML configuration file. Create an <code>evals.yaml</code> file:</p> <pre><code>constants:\n  judge_api_key: your-judge-api-key\n  judge_base_url: https://openrouter.ai/api/v1\n  judge_name: mistralai/mistral-large-2411\n  tasks:\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n    - name: mcqa_single_answer\n      num_fewshot: 2\n      max_tokens: 1000\n\nwandb:\n  enabled: true\n  project: eve-evaluations\n  entity: your-wandb-entity\n  run_name: my-evaluation\n  api_key: your-wandb-api-key\n\nmodels:\n  - name: your-model-name\n    base_url: https://api.provider.com/v1/chat/completions\n    api_key: your-api-key\n    temperature: 0.1\n    num_concurrent: 5\n    timeout: 180\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre> <p>Run the evaluation:</p> <pre><code>python evaluate.py evals.yaml\n</code></pre>"},{"location":"getting_started/#configuration-file-structure","title":"Configuration File Structure","text":""},{"location":"getting_started/#constants-section","title":"Constants Section","text":"<p>Define reusable values that can be referenced throughout the config using <code>!ref</code>:</p> <pre><code>constants:\n  judge_api_key: your-judge-api-key\n  judge_base_url: https://openrouter.ai/api/v1\n  judge_name: mistralai/mistral-large-2411\n  hf_token: your-huggingface-token  # Optional: for private datasets\n\n  tasks:\n    - name: task_name\n      num_fewshot: 0\n      max_tokens: 1000\n      judge_api_key: !ref judge_api_key  # Reference to constant\n      judge_base_url: !ref judge_base_url\n      judge_name: !ref judge_name\n</code></pre>"},{"location":"getting_started/#tasks-configuration","title":"Tasks Configuration","text":"<p>Each task can have the following parameters:</p> <pre><code>tasks:\n  - name: task_name              # Required: Task identifier\n    num_fewshot: 0               # Number of few-shot examples (default: 0)\n    max_tokens: 1000             # Maximum tokens for generation (default: 512)\n    temperature: 0.0             # Sampling temperature (default: 0.0)\n    limit: 100                   # Optional: Limit number of samples to evaluate\n    judge_api_key: api-key       # Required for LLM-as-judge tasks\n    judge_base_url: base-url     # Required for LLM-as-judge tasks\n    judge_name: model-name       # Required for LLM-as-judge tasks\n</code></pre>"},{"location":"getting_started/#models-configuration","title":"Models Configuration","text":"<p>Configure one or more models to evaluate:</p> <pre><code>models:\n  - name: model-identifier\n    base_url: https://api.provider.com/v1/chat/completions\n    api_key: your-api-key\n    temperature: 0.1\n    num_concurrent: 5      # Concurrent API requests (default: 3)\n    timeout: 180          # Request timeout in seconds (default: 300)\n    tasks: !ref tasks     # Reference to tasks list\n</code></pre>"},{"location":"getting_started/#weights-biases-wandb-logging","title":"Weights &amp; Biases (WandB) Logging","text":"<p>Enable experiment tracking with WandB:</p> <pre><code>wandb:\n  enabled: true                    # Enable/disable WandB logging\n  project: project-name            # WandB project name\n  entity: organization-name        # WandB entity/organization\n  run_name: custom-run-name       # Optional: Custom run name prefix\n  api_key: your-wandb-api-key     # WandB API key\n</code></pre> <p>When enabled, the evaluation will log: - Evaluation metrics (accuracy, F1, IoU, etc.) - Individual sample predictions - Task configurations - Model metadata - Evaluation duration and timestamps</p>"},{"location":"getting_started/#output-directory","title":"Output Directory","text":"<p>Specify where evaluation results should be saved:</p> <pre><code>output_dir: evals_outputs  # Default: eval_results\n</code></pre>"},{"location":"getting_started/#example-configurations","title":"Example Configurations","text":""},{"location":"getting_started/#example-1-eve-earth-observation-tasks","title":"Example 1: EVE Earth Observation Tasks","text":"<p>Evaluate a model on Earth Observation-specific tasks:</p> <pre><code>constants:\n  judge_api_key: sk-or-v1-xxxxx\n  judge_base_url: https://openrouter.ai/api/v1\n  judge_name: mistralai/mistral-large-2411\n\n  tasks:\n    - name: eo_summarization\n      num_fewshot: 0\n      max_tokens: 20000\n      judge_api_key: !ref judge_api_key\n      judge_base_url: !ref judge_base_url\n      judge_name: !ref judge_name\n\n    - name: is_mcqa\n      num_fewshot: 2\n      max_tokens: 10000\n\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n\n    - name: open_ended\n      num_fewshot: 5\n      max_tokens: 40000\n      judge_api_key: !ref judge_api_key\n      judge_base_url: !ref judge_base_url\n      judge_name: !ref judge_name\n\nwandb:\n  enabled: true\n  project: eve-evaluations\n  entity: LLM4EO\n  run_name: eve-model-v1\n\nmodels:\n  - name: eve-esa/eve_v0.1\n    base_url: https://api.runpod.ai/v2/endpoint-id/openai/v1/chat/completions\n    api_key: your-runpod-api-key\n    temperature: 0.1\n    num_concurrent: 10\n    timeout: 600\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre>"},{"location":"getting_started/#example-2-using-lm-eval-harness-tasks","title":"Example 2: Using LM-Eval-Harness Tasks","text":"<p>EVE-evaluation supports all tasks from lm-evaluation-harness. Here's an example using MMLU-Pro:</p> <pre><code>constants:\n  tasks:\n    - name: mmlu_pro\n      num_fewshot: 5\n      max_tokens: 1000\n\n    - name: gsm8k\n      num_fewshot: 8\n      max_tokens: 512\n\n    - name: hellaswag\n      num_fewshot: 10\n      max_tokens: 100\n\nmodels:\n  - name: gpt-4\n    base_url: https://api.openai.com/v1/chat/completions\n    api_key: your-openai-api-key\n    temperature: 0.0\n    num_concurrent: 3\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre>"},{"location":"getting_started/#example-3-mixed-eve-and-standard-tasks","title":"Example 3: Mixed EVE and Standard Tasks","text":"<p>Combine Earth Observation tasks with standard benchmarks:</p> <pre><code>constants:\n  judge_api_key: your-judge-api-key\n  judge_base_url: https://openrouter.ai/api/v1\n  judge_name: mistralai/mistral-large-2411\n\n  tasks:\n    # EVE Earth Observation Tasks\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n\n    - name: eo_summarization\n      num_fewshot: 0\n      max_tokens: 20000\n      judge_api_key: !ref judge_api_key\n      judge_base_url: !ref judge_base_url\n      judge_name: !ref judge_name\n\n    # Standard Benchmark Tasks\n    - name: mmlu_pro\n      num_fewshot: 5\n      max_tokens: 1000\n\n    - name: arc_challenge\n      num_fewshot: 25\n      max_tokens: 100\n\nwandb:\n  enabled: true\n  project: comprehensive-eval\n  entity: your-org\n\nmodels:\n  - name: your-model\n    base_url: https://api.provider.com/v1/chat/completions\n    api_key: your-api-key\n    temperature: 0.1\n    num_concurrent: 5\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre>"},{"location":"getting_started/#example-4-multiple-models","title":"Example 4: Multiple Models","text":"<p>Evaluate multiple models on the same tasks:</p> <pre><code>constants:\n  tasks:\n    - name: hallucination_detection\n      num_fewshot: 0\n      max_tokens: 100\n\n    - name: mcqa_single_answer\n      num_fewshot: 2\n      max_tokens: 1000\n\nwandb:\n  enabled: true\n  project: model-comparison\n\nmodels:\n  - name: model-a\n    base_url: https://api.provider-a.com/v1/chat/completions\n    api_key: api-key-a\n    temperature: 0.1\n    num_concurrent: 5\n    tasks: !ref tasks\n\n  - name: model-b\n    base_url: https://api.provider-b.com/v1/chat/completions\n    api_key: api-key-b\n    temperature: 0.1\n    num_concurrent: 5\n    tasks: !ref tasks\n\noutput_dir: evals_outputs\n</code></pre>"},{"location":"getting_started/#output-structure","title":"Output Structure","text":"<p>After running evaluations, results are saved organized by task, then by model:</p> <pre><code>{output_dir}/\n\u251c\u2500\u2500 {task_name_1}/\n\u2502   \u251c\u2500\u2500 {model_name_sanitized}/\n\u2502   \u2502   \u251c\u2500\u2500 results_{timestamp}.json\n\u2502   \u2502   \u2514\u2500\u2500 samples_{task_name}_{timestamp}.jsonl\n\u2502   \u251c\u2500\u2500 {another_model_name_sanitized}/\n\u2502   \u2502   \u251c\u2500\u2500 results_{timestamp}.json\n\u2502   \u2502   \u2514\u2500\u2500 samples_{task_name}_{timestamp}.jsonl\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 {task_name_2}/\n\u2502   \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"getting_started/#example-structure","title":"Example Structure:","text":"<pre><code>evals_outputs/\n\u251c\u2500\u2500 hallucination_detection/\n\u2502   \u251c\u2500\u2500 eve-esa__eve_v0.1/\n\u2502   \u2502   \u251c\u2500\u2500 results_2025-12-01T10-17-45.479920.json\n\u2502   \u2502   \u2514\u2500\u2500 samples_hallucination_detection_2025-12-01T10-17-45.479920.jsonl\n\u2502   \u2514\u2500\u2500 gpt-4/\n\u2502       \u251c\u2500\u2500 results_2025-12-01T10-20-15.123456.json\n\u2502       \u2514\u2500\u2500 samples_hallucination_detection_2025-12-01T10-20-15.123456.jsonl\n\u251c\u2500\u2500 mcqa_single_answer/\n\u2502   \u251c\u2500\u2500 eve-esa__eve_v0.1/\n\u2502   \u2502   \u251c\u2500\u2500 results_2025-12-01T11-23-12.123456.json\n\u2502   \u2502   \u2514\u2500\u2500 samples_mcqa_single_answer_2025-12-01T11-23-12.123456.jsonl\n\u2502   \u2514\u2500\u2500 gpt-4/\n\u2502       \u251c\u2500\u2500 results_2025-12-01T11-25-30.789012.json\n\u2502       \u2514\u2500\u2500 samples_mcqa_single_answer_2025-12-01T11-25-30.789012.jsonl\n\u2514\u2500\u2500 eo_summarization/\n    \u251c\u2500\u2500 eve-esa__eve_v0.1/\n    \u2502   \u251c\u2500\u2500 results_2025-12-01T12-34-56.789012.json\n    \u2502   \u2514\u2500\u2500 samples_eo_summarization_2025-12-01T12-34-56.789012.jsonl\n    \u2514\u2500\u2500 gpt-4/\n        \u251c\u2500\u2500 results_2025-12-01T12-40-10.456789.json\n        \u2514\u2500\u2500 samples_eo_summarization_2025-12-01T12-40-10.456789.jsonl\n</code></pre> <p>This structure makes it easy to compare multiple models on the same task.</p>"},{"location":"getting_started/#results-file-format","title":"Results File Format","text":"<p>The <code>results_{timestamp}.json</code> file contains:</p> <pre><code>{\n  \"results\": {\n    \"task_name\": {\n      \"alias\": \"task_name\",\n      \"metric_1,none\": 0.85,\n      \"metric_1_stderr,none\": 0.02,\n      \"metric_2,none\": 0.78,\n      \"metric_2_stderr,none\": 0.03\n    }\n  },\n  \"group_subtasks\": {},\n  \"configs\": {\n    \"task_name\": {\n      \"task\": \"task_name\",\n      \"dataset_path\": \"dataset-path\",\n      \"num_fewshot\": 0,\n      \"metadata\": {}\n    }\n  },\n  \"versions\": {},\n  \"n-shot\": {},\n  \"n-samples\": {},\n  \"config\": {},\n  \"git_hash\": \"abc123\",\n  \"date\": 1701234567.89,\n  \"total_evaluation_time_seconds\": \"123.45\"\n}\n</code></pre>"},{"location":"getting_started/#samples-file-format","title":"Samples File Format","text":"<p>The <code>samples_{task_name}_{timestamp}.jsonl</code> file contains individual predictions:</p> <pre><code>{\"doc_id\": 0, \"doc\": {...}, \"target\": \"expected\", \"arguments\": [...], \"resps\": [[\"predicted\"]], \"filtered_resps\": [\"predicted\"], \"doc_hash\": \"abc123\", \"prompt_hash\": \"def456\", \"task_name\": \"task_name\"}\n{\"doc_id\": 1, \"doc\": {...}, \"target\": \"expected\", \"arguments\": [...], \"resps\": [[\"predicted\"]], \"filtered_resps\": [\"predicted\"], \"doc_hash\": \"ghi789\", \"prompt_hash\": \"jkl012\", \"task_name\": \"task_name\"}\n...\n</code></pre> <p>Each line contains: - <code>doc</code>: The input document/question - <code>target</code>: Expected answer - <code>resps</code>: Raw model response - <code>filtered_resps</code>: Processed model response - Metadata for reproducibility</p>"},{"location":"getting_started/#wandb-integration","title":"WandB Integration","text":"<p>When WandB logging is enabled, the following information is automatically logged:</p>"},{"location":"getting_started/#metrics-logged","title":"Metrics Logged","text":"<ul> <li>Aggregate Metrics: Final scores for each metric (accuracy, F1, IoU, etc.)</li> <li>Per-Sample Metrics: Individual predictions and correctness</li> <li>Task Metadata: Dataset paths, splits, versions</li> <li>Model Configuration: API endpoints, temperatures, timeouts</li> <li>Evaluation Metadata: Git hash, timestamps, duration</li> </ul>"},{"location":"getting_started/#viewing-results","title":"Viewing Results","text":"<p>After evaluation completes, visit your WandB project to:</p> <ol> <li>Compare Models: View metrics across different models side-by-side</li> <li>Analyze Samples: Inspect individual predictions and failures</li> <li>Track Progress: Monitor evaluation progress in real-time</li> <li>Visualize Trends: Plot metric distributions and comparisons</li> </ol>"},{"location":"getting_started/#example-wandb-output","title":"Example WandB Output","text":"<pre><code>Run: eve-model-v1-20251201\n\u251c\u2500\u2500 Summary Metrics\n\u2502   \u251c\u2500\u2500 hallucination_detection/acc: 0.822\n\u2502   \u251c\u2500\u2500 hallucination_detection/f1: 0.841\n\u2502   \u251c\u2500\u2500 hallucination_detection/precision: 0.869\n\u2502   \u251c\u2500\u2500 mcqa_single_answer/acc: 0.756\n\u2502   \u2514\u2500\u2500 eo_summarization/llm_judge: 0.834\n\u251c\u2500\u2500 Config\n\u2502   \u251c\u2500\u2500 model: eve-esa/eve_v0.1\n\u2502   \u251c\u2500\u2500 temperature: 0.1\n\u2502   \u2514\u2500\u2500 num_concurrent: 10\n\u2514\u2500\u2500 Samples\n    \u251c\u2500\u2500 hallucination_detection_samples.csv\n    \u251c\u2500\u2500 mcqa_single_answer_samples.csv\n    \u2514\u2500\u2500 eo_summarization_samples.csv\n</code></pre>"},{"location":"getting_started/#advanced-usage","title":"Advanced Usage","text":""},{"location":"getting_started/#using-environment-variables","title":"Using Environment Variables","text":"<p>Instead of hardcoding API keys, use environment variables:</p> <pre><code>constants:\n  judge_api_key: ${JUDGE_API_KEY}\n\nmodels:\n  - name: my-model\n    api_key: ${MODEL_API_KEY}\n    tasks: !ref tasks\n\nwandb:\n  enabled: true\n  api_key: ${WANDB_API_KEY}\n</code></pre> <p>Set them before running:</p> <pre><code>export JUDGE_API_KEY=your-judge-key\nexport MODEL_API_KEY=your-model-key\nexport WANDB_API_KEY=your-wandb-key\npython evaluate.py evals.yaml\n</code></pre>"},{"location":"getting_started/#limiting-samples-for-testing","title":"Limiting Samples for Testing","text":"<p>Test your configuration on a small subset:</p> <pre><code>tasks:\n  - name: hallucination_detection\n    num_fewshot: 0\n    max_tokens: 100\n    limit: 10  # Only evaluate first 10 samples\n</code></pre>"},{"location":"getting_started/#direct-command-line","title":"Direct Command Line","text":"<p>For quick tests, you can use the lm_eval command directly:</p> <pre><code>lm_eval --model openai-chat-completions \\\n        --model_args base_url=https://api.provider.com,model=model-name,num_concurrent=5 \\\n        --tasks hallucination_detection,mcqa_single_answer \\\n        --include tasks \\\n        --num_fewshot 0 \\\n        --output_path ./outputs \\\n        --log_samples \\\n        --apply_chat_template\n</code></pre>"},{"location":"getting_started/#available-tasks","title":"Available Tasks","text":""},{"location":"getting_started/#eve-earth-observation-tasks","title":"EVE Earth Observation Tasks","text":"<p>See the EO Tasks page for detailed information about: - <code>eo_summarization</code> - <code>mcqa_multiple_answer</code> - <code>mcqa_single_answer</code> - <code>open_ended</code> - <code>open_ended_w_context</code> - <code>refusal</code> - <code>hallucination_detection</code></p>"},{"location":"getting_started/#lm-evaluation-harness-tasks","title":"LM-Evaluation-Harness Tasks","text":"<p>All tasks from the lm-evaluation-harness are supported, including:</p> <p>Popular Benchmarks: - <code>mmlu_pro</code> - MMLU-Pro (challenging multiple-choice) - <code>gsm8k</code> - Grade School Math - <code>hellaswag</code> - Commonsense reasoning - <code>arc_challenge</code> - AI2 Reasoning Challenge - <code>truthfulqa</code> - Truthfulness evaluation - <code>winogrande</code> - Commonsense reasoning - <code>piqa</code> - Physical commonsense - And more...</p> <p>To list all available tasks:</p> <pre><code>lm_eval --tasks list\n</code></pre>"},{"location":"getting_started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting_started/#common-issues","title":"Common Issues","text":"<p>1. API Timeout Errors</p> <p>Increase the timeout value:</p> <pre><code>models:\n  - name: your-model\n    timeout: 600  # Increase to 10 minutes\n</code></pre> <p>2. Rate Limiting</p> <p>Reduce concurrent requests:</p> <pre><code>models:\n  - name: your-model\n    num_concurrent: 1  # Reduce concurrency\n</code></pre> <p>3. Judge Model Errors</p> <p>Ensure judge credentials are set for tasks that require them:</p> <pre><code>tasks:\n  - name: open_ended\n    judge_api_key: !ref judge_api_key  # Required!\n    judge_base_url: !ref judge_base_url\n    judge_name: !ref judge_name\n</code></pre> <p>4. WandB Login Issues</p> <p>Login before running:</p> <pre><code>wandb login your-api-key\n</code></pre>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Tasks: Check out the EO Tasks page for details on Earth Observation evaluation tasks</li> </ul>"},{"location":"getting_started/#support","title":"Support","text":"<p>For issues or questions: - GitHub Issues: eve-esa/eve-evaluation - Documentation: https://docs.eve-evaluation.org - LM-Eval-Harness: EleutherAI Documentation</p>"}]}